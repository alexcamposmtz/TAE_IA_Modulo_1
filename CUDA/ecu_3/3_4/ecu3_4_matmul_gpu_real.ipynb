{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a05fc474-bf75-4221-a6f0-8195830af987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ECU3.4 - Matrix Multiply Naive (GPU REAL)\n",
      "Autor: Alejandro Campos Martínez - Team 6\n",
      "======================================================================\n",
      "GPU detectada: b'NVIDIA GeForce RTX 4060 Laptop GPU'\n",
      "Compute Capability: (8, 9)\n",
      "Multiprocessors: 24\n",
      "----------------------------------------------------------------------\n",
      "ADVERTENCIA: Este kernel naive es ineficiente por diseño\n",
      "Tamaño de matrices: A(1000x1000) @ B(1000x1000) = C(1000x1000)\n",
      "Total elementos resultado: 1,000,000\n",
      "Operaciones: 1,000,000,000 multiplicaciones\n",
      "Configuración: (63, 63) bloques\n",
      "               (16, 16) threads por bloque\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "GPU kernel time: 23.8893 ms\n",
      "CPU NumPy time: 4.7107 ms\n",
      "Slowdown: 5.07x\n",
      "NOTA: Este resultado demuestra la importancia de optimizaciones\n",
      "      NumPy usa BLAS con shared memory, cache blocking, etc.\n",
      "Verificación correcta: True\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ECU3.4 - Matrix Multiply Naive\n",
    "Versión: GPU REAL (Requiere NVIDIA GPU + CUDA Toolkit)\n",
    "Team 6\n",
    "Autor: Alejandro Campos Martínez\n",
    "Curso: TAE en IA - COCYTEN Nayarit\n",
    "Hardware: NVIDIA RTX 4060 Laptop GPU\n",
    "Propósito: Multiplicación de matrices naive (sin optimizaciones) para demostrar\n",
    "           la importancia de optimizaciones como shared memory\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import math\n",
    "import time\n",
    "from numba import config\n",
    "\n",
    "config.CUDA_ENABLE_PYNVJITLINK = 1\n",
    "\n",
    "@cuda.jit\n",
    "def matmul_naive_kernel(A, B, C):\n",
    "    \"\"\"\n",
    "    Naive matrix multiply: C = A @ B\n",
    "    Each thread computes one element of C.\n",
    "    \"\"\"\n",
    "    row, col = cuda.grid(2)\n",
    "\n",
    "    M, K = A.shape\n",
    "    K2, N = B.shape\n",
    "\n",
    "    if row < M and col < N:\n",
    "        total = 0.0\n",
    "        for k in range(K):\n",
    "            total += A[row, k] * B[k, col]\n",
    "        C[row, col] = total\n",
    "\n",
    "def main():\n",
    "    print(\"=\"*70)\n",
    "    print(\"ECU3.4 - Matrix Multiply Naive (GPU REAL)\")\n",
    "    print(\"Autor: Alejandro Campos Martínez - Team 6\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    gpu = cuda.get_current_device()\n",
    "    print(f\"GPU detectada: {gpu.name}\")\n",
    "    print(f\"Compute Capability: {gpu.compute_capability}\")\n",
    "    print(f\"Multiprocessors: {gpu.MULTIPROCESSOR_COUNT}\")\n",
    "    print(\"-\"*70)\n",
    "    print(\"ADVERTENCIA: Este kernel naive es ineficiente por diseño\")\n",
    "    \n",
    "    M, K, N = 1000, 1000, 1000\n",
    "    print(f\"Tamaño de matrices: A({M}x{K}) @ B({K}x{N}) = C({M}x{N})\")\n",
    "    print(f\"Total elementos resultado: {M*N:,}\")\n",
    "    print(f\"Operaciones: {M*N*K:,} multiplicaciones\")\n",
    "    \n",
    "    A = np.random.randn(M, K).astype(np.float32)\n",
    "    B = np.random.randn(K, N).astype(np.float32)\n",
    "    C = np.zeros((M, N), dtype=np.float32)\n",
    "\n",
    "    threads_per_block = (16, 16)\n",
    "    d_A = cuda.to_device(A)\n",
    "    d_B = cuda.to_device(B)\n",
    "    d_C = cuda.to_device(C)\n",
    "\n",
    "    blocks_per_grid_x = (M + threads_per_block[0] - 1) // threads_per_block[0]\n",
    "    blocks_per_grid_y = (N + threads_per_block[1] - 1) // threads_per_block[1]\n",
    "    blocks_per_grid = (blocks_per_grid_x, blocks_per_grid_y)\n",
    "    \n",
    "    print(f\"Configuración: ({blocks_per_grid_x}, {blocks_per_grid_y}) bloques\")\n",
    "    print(f\"               ({threads_per_block[0]}, {threads_per_block[1]}) threads por bloque\")\n",
    "    print(\"-\"*70)\n",
    "\n",
    "    # Warmup\n",
    "    matmul_naive_kernel[blocks_per_grid, threads_per_block](d_A, d_B, d_C)\n",
    "    cuda.synchronize()\n",
    "\n",
    "    # GPU timing\n",
    "    start = time.time()\n",
    "    matmul_naive_kernel[blocks_per_grid, threads_per_block](d_A, d_B, d_C)\n",
    "    cuda.synchronize()\n",
    "    gpu_time = (time.time() - start) * 1000\n",
    "\n",
    "    C_gpu = d_C.copy_to_host()\n",
    "\n",
    "    # CPU timing\n",
    "    cpu_start = time.time()\n",
    "    C_cpu = A @ B\n",
    "    cpu_time = (time.time() - cpu_start) * 1000\n",
    "\n",
    "    print(f\"\\nGPU kernel time: {gpu_time:.4f} ms\")\n",
    "    print(f\"CPU NumPy time: {cpu_time:.4f} ms\")\n",
    "    \n",
    "    if cpu_time < gpu_time:\n",
    "        print(f\"Slowdown: {gpu_time / cpu_time:.2f}x\")\n",
    "        print(\"NOTA: Este resultado demuestra la importancia de optimizaciones\")\n",
    "        print(\"      NumPy usa BLAS con shared memory, cache blocking, etc.\")\n",
    "    else:\n",
    "        print(f\"Speedup: {cpu_time / gpu_time:.2f}x\")\n",
    "    \n",
    "    print(f\"Verificación correcta: {np.allclose(C_gpu, C_cpu, atol=1e-3)}\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dc97f9-ba80-42e1-b69c-df701a3c0c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289f68c0-e018-4089-8e4d-4b76c91d2263",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
